# Pyrl Language Model Training Configuration
# ============================================

# Model configuration
model_name: "gpt2"  # Base model (gpt2, gpt2-medium, gpt2-large, or custom)
max_length: 512

# Dataset
dataset_path: "./training/dataset.jsonl"
train_val_split: 0.9

# LoRA configuration
use_lora: true
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1
lora_target_modules:
  - "q_proj"
  - "v_proj"
  - "k_proj"
  - "o_proj"

# Training configuration
epochs: 3
batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
weight_decay: 0.01
warmup_steps: 100
max_grad_norm: 1.0

# Evaluation and checkpointing
eval_steps: 100
save_steps: 100
save_total_limit: 3

# Output directories
output_dir: "./models/pyrl-model"
logging_dir: "./logs"
report_to: "tensorboard"  # tensorboard, wandb, none

# Advanced settings
fp16: true
gradient_checkpointing: true
optim: "adamw_torch"

# Generation settings (for inference)
generation:
  max_new_tokens: 200
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
