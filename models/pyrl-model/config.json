{
  "model_type": "pyrl-transformer",
  "architectures": [
    "PyrlForCausalLM"
  ],
  "vocab_size": 13078,
  "hidden_size": 768,
  "intermediate_size": 3072,
  "num_hidden_layers": 12,
  "num_attention_heads": 12,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "max_position_embeddings": 512,
  "pad_token_id": 0,
  "bos_token_id": 2,
  "eos_token_id": 3,
  "trained": true,
  "training_history": [
    {
      "epoch": 1,
      "train_loss": 1.8985647142903341,
      "val_loss": 1.958180098905719
    },
    {
      "epoch": 2,
      "train_loss": 1.8306996533212743,
      "val_loss": 1.8903150379366591
    },
    {
      "epoch": 3,
      "train_loss": 1.5533233141042575,
      "val_loss": 1.6129386987196423
    }
  ],
  "best_val_loss": 1.6129386987196423,
  "grammar_features_enabled": true
}